\section{Data Analysis}
Wrapping around the plug-in model of our system there are some auxiliary scripts,
these scripts follow and automates all the simulation, from the input (building up
the simulation scenario) to the output (analyzing the data and return the simulation
summary).

\subsection{Netbuilder and Launcher Scripts}
As described in \ref{sec:Shadow}
The Shadow simulator use a XML file to describe the network,
we automatize this process with a python script called the net-builder script.\\
This script will so build a scenario based on some input parameters as:
\begin{itemize}
\item The number of TOR relays in the simulation.
\item The number of TOR exit nodes in the simulation.
\item The number of TOR 4authorities\footnote{A 4 Authority node is simply the
database that keep track of the state of the TOR network and the list
of the TOR relays/exit-nodes} nodes in the simulation.
\item The number of clients (simpletcp) of the simulation.
\item The number of servers (simpletcp) of the simulation.
\item The percentage of clients tracked by an autosys plug-in.
\item The percentage of servers tracked by an autosys plug-in.
\item The density of the network-requests.
\end{itemize}

When the others parameters are self-esplicative, the density parameter is somewhat
that deserves more that two words on.
This parameter specify the time interval between every connection made from the
simpletcp plug-in to emulate the behaviour of different type of networks.
We defined three behaviours to our network study:

\begin{itemize}
\item Slow:\\minimum: a mean of 800 milliseconds of sleep time between every connection,\\
            maximum: a mean of 2 seconds of sleep time between every connection.
\item Average:\\minimum: a mean of 80 milliseconds of sleep time between every connection,\\
               maximum: a mean of 1 second of sleep time between every connection.
\item Fast:\\minimum: a mean of 20 milliseconds of sleep time between every connection\\
            maximum: a mean of 100 milliseconds of sleep time between every connection.
\end{itemize}

This normal distributed values are computed for every simpletcp client
(which will compute a random uniform distributed value as described in \ref{sec:simpletcpclient}).\\

The second auxiliary script we used is a bash script that helped us in the
automatic run of the simulations.
This script will run a bunch of simulation using the algorithm \ref{alg:launcher}.

\begin{algorithm}[H]
\caption{Launcher script}
\begin{algorithmic}[2]
\For{($simulation\_run \gets 1$; $simulation\_run <= steps$; $simulation\_run++$)}
	\For{($sim\_id \gets 1$; $sim\_id <= simulations\_per\_step$; $sim\_id++$)}
		\ForAll {$density$ $in$ $(slow, fast, average)$}
		\If {The client trace percentage is not fixed}
			\State $client\_trace\_value \gets sim\_id/simulations\_per\_step$
		\EndIf
		\If {The server trace percentage is not fixed}
			\State $server\_trace\_value \gets sim\_id/simulations\_per\_step$
		\EndIf
		\If{A configuration is present for $<sim\_id, density>$ And the percentages are fixed}
			\State Use the previous configuration
		\Else
			\State Generate a new configuration with net-builder
		\EndIf
		\State Launch the Shadow Simulator with the appropriate configuration.
		\EndFor
	\EndFor
\EndFor
\end{algorithmic}
\label{alg:launcher}
\end{algorithm}

From this algorithm we can see that we can use this script to create a set of
simulations with some linear modifications to the numbers of the traced clients
or the number of the traced servers (or a combination of the two).
We can also declare more than one run on this bunch of simulations, this can
be useful to compute the mean values from the same configuration file, else we
can generate a new configuration file starting from some input parameters.

After the launch we can pass the simulation raw data to the analyzer script.

\subsection{Analyzer Script}
The analyzer script takes a log file, generated previously by the logger
server, as input and tries to
ascertain which clients were communicating with which server during the
simulation. The log file is a list of entries that are formatted as described in
 figure \ref{fig:analyzer_pack_struct} and the figure
\ref{fig:example_log_file} shows an example of it.
\begin{figure}[H]
\centering
\begin{lstlisting}[language=bash,frame=single]
...
c;client3;1414557713
c;client5;1414847666
s;server3;1415099735
c;client3;1415390384
c;client5;1415536475
c;client7;1415618044
s;server3;1415788544
c;client0;1415817129
s;server2;1416118377
...
\end{lstlisting}
\caption{Analyzer log file fragment example.}
\label{fig:example_log_file}
\end{figure}
The script scans the log file and for each line check whatever the entry is
related to a client or a server. If the entry refers to a client
connection request it scans in a nestled loop the server connection entries
that may be related to that client. The nested loop does not scan the whole
file until its end, but it stops after a sufficient amount of
read entries. In particular, the amount of entries to be read is
measured in time: from a client entry, the scan process goes
ahead until the difference between the time-stamp of the current scanned 
entry and the time-stamp of the starting client entry is not greater
than a fixed threshold $thr_{max}$. For our experiment we chose a limit of 6
seconds as it is an enough high latency
value for a connection acceptance. Probably the average latency for a
connection response, even passing through the Tor network, is
considerably lesser than 6 seconds but anyway we can over-esteem a bit this
value without unexpected bad consequences. On the other hand, an under-esteem of
this value would threaten the analysis results. 

Furthermore, from a client
entry the nestled scan starts reading from the entry that is 100 ms far
from the client start point. This lower threshold, $thr_{min}$, is needed in order to
 filter those server entries that registered a connection time-stamp
that results too young for being related to the client start point. This
lower limit is more critical: a too low limit would guarantee false
positive results and a too high limit would drop the right results.
We will see later that after some tests, a value around 100ms seemed to maximize 
the clients servers relation matching. %TODO in empirical results

The analysis proceeds by recording a set of possible server candidates 
 per each client. Each server candidate assumes a $pmatch$ value that
indicates how much a server candidate can be the real server that
established a connection with the client. The $pmatch$ value is defined
as following:
\begin{equation}
pmatch = 1 - \frac{\Delta_t - thr_{min}}{thr_{max}}
\end{equation}
where $\Delta_t$ is the time distance between the server entry time-stamp and
the client entry time-stamp.
%TODO, example of candidate list with the figure, talk about the
%average, and real stat comparison
\subsection{Empirical Results}
